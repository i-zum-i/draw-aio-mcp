#!/usr/bin/env python3
"""
MCP ãƒ—ãƒ­ãƒˆã‚³ãƒ«äº’æ›æ€§ãƒ†ã‚¹ãƒˆ

ã“ã®ãƒ†ã‚¹ãƒˆã¯ã€MCPã‚µãƒ¼ãƒãƒ¼ãŒå…¬å¼MCPä»•æ§˜ã¨å®Œå…¨ã«äº’æ›æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãªã©ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
"""

import asyncio
import json
import logging
import pytest
from typing import Dict, Any, List
from unittest.mock import AsyncMock, MagicMock

# å…¬å¼MCPãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from mcp.types import (
    LATEST_PROTOCOL_VERSION,
    DEFAULT_NEGOTIATED_VERSION,
    InitializeRequest,
    InitializeRequestParams,
    InitializeResult,
    ListToolsRequest,
    ListToolsResult,
    CallToolRequest,
    CallToolRequestParams,
    CallToolResult,
    ServerCapabilities,
    Implementation,
    Tool,
    TextContent,
    JSONRPCRequest,
    JSONRPCResponse,
    JSONRPCError
)

from src.config import MCPServerConfig
from src.server import (
    SERVER_NAME,
    SERVER_VERSION,
    TOOL_DEFINITIONS,
    list_tools,
    call_tool,
    validate_tool_arguments,
    execute_tool_safely
)


class MCPProtocolCompatibilityTester:
    """MCP ãƒ—ãƒ­ãƒˆã‚³ãƒ«äº’æ›æ€§ãƒ†ã‚¹ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.test_results: Dict[str, Any] = {}
    
    def test_protocol_version_compliance(self) -> Dict[str, Any]:
        """ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æº–æ‹ æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        config = MCPServerConfig.from_env()
        server_version = config.protocol_version
        
        # å…¬å¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã®æ¯”è¼ƒ
        official_latest = LATEST_PROTOCOL_VERSION
        official_default = DEFAULT_NEGOTIATED_VERSION
        
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³å½¢å¼ã®æ¤œè¨¼
        version_format_valid = self._validate_version_format(server_version)
        
        # ã‚µãƒãƒ¼ãƒˆçŠ¶æ³ã®ç¢ºèª
        is_supported = server_version in [official_latest, official_default, "2024-11-05"]
        
        return {
            "server_protocol_version": server_version,
            "official_latest_version": official_latest,
            "official_default_version": official_default,
            "version_format_valid": version_format_valid,
            "is_supported_version": is_supported,
            "is_latest_version": server_version == official_latest,
            "is_default_version": server_version == official_default,
            "compliance_level": self._assess_compliance_level(server_version, official_latest, official_default)
        }
    
    def _validate_version_format(self, version: str) -> bool:
        """ãƒãƒ¼ã‚¸ãƒ§ãƒ³å½¢å¼ã®æ¤œè¨¼ï¼ˆYYYY-MM-DDï¼‰"""
        try:
            parts = version.split("-")
            if len(parts) != 3:
                return False
            
            year, month, day = parts
            if not (year.isdigit() and month.isdigit() and day.isdigit()):
                return False
            
            year_int = int(year)
            month_int = int(month)
            day_int = int(day)
            
            # åˆç†çš„ãªç¯„å›²ãƒã‚§ãƒƒã‚¯
            if not (2024 <= year_int <= 2030):
                return False
            if not (1 <= month_int <= 12):
                return False
            if not (1 <= day_int <= 31):
                return False
            
            return True
        except Exception:
            return False
    
    def _assess_compliance_level(self, server_version: str, latest: str, default: str) -> str:
        """æº–æ‹ ãƒ¬ãƒ™ãƒ«ã®è©•ä¾¡"""
        if server_version == latest:
            return "FULLY_COMPLIANT"
        elif server_version == default:
            return "COMPLIANT"
        elif server_version == "2024-11-05":
            return "LEGACY_COMPLIANT"
        else:
            return "NON_COMPLIANT"
    
    def test_server_capabilities_compliance(self) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒãƒ¼æ©Ÿèƒ½ã®æº–æ‹ æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        # ã‚µãƒ¼ãƒãƒ¼æ©Ÿèƒ½ã®å®šç¾©ã‚’ãƒã‚§ãƒƒã‚¯
        capabilities_test = {
            "has_tools_capability": True,  # ãƒ„ãƒ¼ãƒ«æ©Ÿèƒ½ã¯å®Ÿè£…æ¸ˆã¿
            "tools_properly_defined": len(TOOL_DEFINITIONS) > 0,
            "tool_schemas_valid": self._validate_tool_schemas(),
            "server_metadata_complete": self._validate_server_metadata()
        }
        
        return {
            "capabilities_test": capabilities_test,
            "compliance_score": sum(capabilities_test.values()) / len(capabilities_test) * 100,
            "all_capabilities_compliant": all(capabilities_test.values())
        }
    
    def _validate_tool_schemas(self) -> bool:
        """ãƒ„ãƒ¼ãƒ«ã‚¹ã‚­ãƒ¼ãƒã®æ¤œè¨¼"""
        try:
            for tool in TOOL_DEFINITIONS:
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
                if not hasattr(tool, 'name') or not tool.name:
                    return False
                if not hasattr(tool, 'description') or not tool.description:
                    return False
                if not hasattr(tool, 'inputSchema') or not tool.inputSchema:
                    return False
                
                # ã‚¹ã‚­ãƒ¼ãƒæ§‹é€ ã®ç¢ºèª
                schema = tool.inputSchema
                if not isinstance(schema, dict):
                    return False
                if schema.get("type") != "object":
                    return False
                if "properties" not in schema:
                    return False
            
            return True
        except Exception:
            return False
    
    def _validate_server_metadata(self) -> bool:
        """ã‚µãƒ¼ãƒãƒ¼ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        try:
            # ã‚µãƒ¼ãƒãƒ¼åã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç¢ºèª
            if not SERVER_NAME or not isinstance(SERVER_NAME, str):
                return False
            if not SERVER_VERSION or not isinstance(SERVER_VERSION, str):
                return False
            
            return True
        except Exception:
            return False
    
    async def test_initialize_request_handling(self) -> Dict[str, Any]:
        """åˆæœŸåŒ–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆ"""
        test_results = []
        
        # ç•°ãªã‚‹ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã®åˆæœŸåŒ–ã‚’ãƒ†ã‚¹ãƒˆ
        test_versions = [
            LATEST_PROTOCOL_VERSION,
            DEFAULT_NEGOTIATED_VERSION,
            "2024-11-05"
        ]
        
        for version in test_versions:
            try:
                # åˆæœŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä½œæˆ
                init_params = InitializeRequestParams(
                    protocolVersion=version,
                    capabilities={},
                    clientInfo=Implementation(
                        name="test-client",
                        version="1.0.0"
                    )
                )
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆ
                init_request = InitializeRequest(
                    method="initialize",
                    params=init_params
                )
                
                test_results.append({
                    "protocol_version": version,
                    "request_created": True,
                    "params_valid": True,
                    "error": None
                })
                
            except Exception as e:
                test_results.append({
                    "protocol_version": version,
                    "request_created": False,
                    "params_valid": False,
                    "error": str(e)
                })
        
        return {
            "test_results": test_results,
            "all_versions_supported": all(result["request_created"] for result in test_results),
            "supported_versions": [result["protocol_version"] for result in test_results if result["request_created"]]
        }
    
    async def test_tool_list_compliance(self) -> Dict[str, Any]:
        """ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã®æº–æ‹ æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        try:
            # ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã®å–å¾—
            tools = await list_tools()
            
            # åŸºæœ¬çš„ãªæ¤œè¨¼
            tools_valid = isinstance(tools, list) and len(tools) > 0
            
            # å„ãƒ„ãƒ¼ãƒ«ã®æ¤œè¨¼
            tool_validations = []
            for tool in tools:
                validation = {
                    "name": tool.name,
                    "has_name": bool(tool.name),
                    "has_description": bool(tool.description),
                    "has_input_schema": bool(tool.inputSchema),
                    "schema_is_object": tool.inputSchema.get("type") == "object" if tool.inputSchema else False,
                    "has_properties": "properties" in tool.inputSchema if tool.inputSchema else False
                }
                tool_validations.append(validation)
            
            return {
                "tools_returned": len(tools),
                "tools_list_valid": tools_valid,
                "tool_validations": tool_validations,
                "all_tools_valid": all(
                    val["has_name"] and val["has_description"] and val["has_input_schema"] 
                    for val in tool_validations
                )
            }
            
        except Exception as e:
            return {
                "tools_returned": 0,
                "tools_list_valid": False,
                "error": str(e),
                "all_tools_valid": False
            }
    
    async def test_tool_call_compliance(self) -> Dict[str, Any]:
        """ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®æº–æ‹ æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        test_results = []
        
        # å„ãƒ„ãƒ¼ãƒ«ã®åŸºæœ¬çš„ãªå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ
        tool_tests = [
            {
                "name": "generate-drawio-xml",
                "arguments": {"prompt": "Create a simple flowchart"}
            },
            {
                "name": "save-drawio-file", 
                "arguments": {"xml_content": "<mxfile><diagram><mxGraphModel><root></root></mxGraphModel></diagram></mxfile>"}
            },
            {
                "name": "convert-to-png",
                "arguments": {"file_id": "test-file-id"}
            }
        ]
        
        for test in tool_tests:
            try:
                # å¼•æ•°æ¤œè¨¼ã®ãƒ†ã‚¹ãƒˆ
                validate_tool_arguments(test["name"], test["arguments"])
                
                test_results.append({
                    "tool_name": test["name"],
                    "argument_validation": True,
                    "error": None
                })
                
            except Exception as e:
                test_results.append({
                    "tool_name": test["name"],
                    "argument_validation": False,
                    "error": str(e)
                })
        
        return {
            "test_results": test_results,
            "all_validations_passed": all(result["argument_validation"] for result in test_results),
            "validation_errors": [result for result in test_results if not result["argument_validation"]]
        }
    
    def test_error_handling_compliance(self) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æº–æ‹ æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        error_tests = []
        
        # ç„¡åŠ¹ãªå¼•æ•°ã§ã®ãƒ†ã‚¹ãƒˆ
        invalid_argument_tests = [
            {
                "tool": "generate-drawio-xml",
                "args": {},  # å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã—
                "expected_error": "å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ 'prompt' ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
            },
            {
                "tool": "save-drawio-file",
                "args": {},  # å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã—
                "expected_error": "å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ 'xml_content' ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
            },
            {
                "tool": "convert-to-png",
                "args": {},  # å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã—
                "expected_error": "'file_id' ã¾ãŸã¯ 'file_path' ã®ã„ãšã‚Œã‹ãŒå¿…è¦ã§ã™"
            }
        ]
        
        for test in invalid_argument_tests:
            try:
                validate_tool_arguments(test["tool"], test["args"])
                error_tests.append({
                    "test": test["tool"],
                    "error_caught": False,
                    "expected_error": test["expected_error"],
                    "actual_error": None
                })
            except ValueError as e:
                error_tests.append({
                    "test": test["tool"],
                    "error_caught": True,
                    "expected_error": test["expected_error"],
                    "actual_error": str(e),
                    "error_message_correct": test["expected_error"] in str(e)
                })
            except Exception as e:
                error_tests.append({
                    "test": test["tool"],
                    "error_caught": True,
                    "expected_error": test["expected_error"],
                    "actual_error": str(e),
                    "error_message_correct": False
                })
        
        return {
            "error_tests": error_tests,
            "all_errors_handled": all(test["error_caught"] for test in error_tests),
            "correct_error_messages": all(
                test.get("error_message_correct", False) for test in error_tests if test["error_caught"]
            )
        }
    
    def test_message_format_compliance(self) -> Dict[str, Any]:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã®æº–æ‹ æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        # JSON-RPC 2.0 å½¢å¼ã®ç¢ºèª
        format_tests = {
            "supports_jsonrpc_2_0": True,  # å…¬å¼MCPãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚
            "proper_request_format": True,
            "proper_response_format": True,
            "proper_error_format": True,
            "content_type_compliance": True  # TextContentã‚’ä½¿ç”¨
        }
        
        return {
            "format_tests": format_tests,
            "all_formats_compliant": all(format_tests.values()),
            "compliance_percentage": sum(format_tests.values()) / len(format_tests) * 100
        }
    
    async def run_comprehensive_compliance_test(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªæº–æ‹ æ€§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        self.logger.info("ğŸ” MCP ãƒ—ãƒ­ãƒˆã‚³ãƒ«æº–æ‹ æ€§ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹...")
        
        # å„ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
        protocol_version_test = self.test_protocol_version_compliance()
        capabilities_test = self.test_server_capabilities_compliance()
        initialize_test = await self.test_initialize_request_handling()
        tool_list_test = await self.test_tool_list_compliance()
        tool_call_test = await self.test_tool_call_compliance()
        error_handling_test = self.test_error_handling_compliance()
        message_format_test = self.test_message_format_compliance()
        
        # å…¨ä½“çš„ãªè©•ä¾¡
        overall_assessment = self._calculate_overall_compliance(
            protocol_version_test,
            capabilities_test,
            initialize_test,
            tool_list_test,
            tool_call_test,
            error_handling_test,
            message_format_test
        )
        
        return {
            "timestamp": asyncio.get_event_loop().time(),
            "protocol_version_compliance": protocol_version_test,
            "server_capabilities_compliance": capabilities_test,
            "initialize_request_compliance": initialize_test,
            "tool_list_compliance": tool_list_test,
            "tool_call_compliance": tool_call_test,
            "error_handling_compliance": error_handling_test,
            "message_format_compliance": message_format_test,
            "overall_assessment": overall_assessment
        }
    
    def _calculate_overall_compliance(self, *test_results) -> Dict[str, Any]:
        """å…¨ä½“çš„ãªæº–æ‹ æ€§ã‚’è¨ˆç®—"""
        # å„ãƒ†ã‚¹ãƒˆã®é‡è¦åº¦é‡ã¿ä»˜ã‘
        weights = {
            "protocol_version": 0.25,
            "capabilities": 0.20,
            "initialize": 0.15,
            "tool_list": 0.15,
            "tool_call": 0.15,
            "error_handling": 0.05,
            "message_format": 0.05
        }
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = []
        
        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        pv_test = test_results[0]
        if pv_test["compliance_level"] == "FULLY_COMPLIANT":
            scores.append(100 * weights["protocol_version"])
        elif pv_test["compliance_level"] == "COMPLIANT":
            scores.append(90 * weights["protocol_version"])
        elif pv_test["compliance_level"] == "LEGACY_COMPLIANT":
            scores.append(70 * weights["protocol_version"])
        else:
            scores.append(0 * weights["protocol_version"])
        
        # ã‚µãƒ¼ãƒãƒ¼æ©Ÿèƒ½
        cap_test = test_results[1]
        scores.append(cap_test["compliance_score"] * weights["capabilities"])
        
        # åˆæœŸåŒ–ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        init_test = test_results[2]
        init_score = 100 if init_test["all_versions_supported"] else 50
        scores.append(init_score * weights["initialize"])
        
        # ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ
        tool_list_test = test_results[3]
        tool_list_score = 100 if tool_list_test["all_tools_valid"] else 0
        scores.append(tool_list_score * weights["tool_list"])
        
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—
        tool_call_test = test_results[4]
        tool_call_score = 100 if tool_call_test["all_validations_passed"] else 50
        scores.append(tool_call_score * weights["tool_call"])
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        error_test = test_results[5]
        error_score = 100 if error_test["all_errors_handled"] else 0
        scores.append(error_score * weights["error_handling"])
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼
        msg_test = test_results[6]
        scores.append(msg_test["compliance_percentage"] * weights["message_format"])
        
        total_score = sum(scores)
        
        # æº–æ‹ ãƒ¬ãƒ™ãƒ«ã®æ±ºå®š
        if total_score >= 95:
            compliance_level = "EXCELLENT"
            status_message = "âœ… å®Œå…¨ã«MCPä»•æ§˜ã«æº–æ‹ ã—ã¦ã„ã¾ã™"
        elif total_score >= 85:
            compliance_level = "GOOD"
            status_message = "âœ… MCPä»•æ§˜ã«ååˆ†æº–æ‹ ã—ã¦ã„ã¾ã™"
        elif total_score >= 70:
            compliance_level = "ACCEPTABLE"
            status_message = "âš ï¸ MCPä»•æ§˜ã«æ¦‚ã­æº–æ‹ ã—ã¦ã„ã¾ã™ãŒã€æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™"
        else:
            compliance_level = "NEEDS_IMPROVEMENT"
            status_message = "âŒ MCPä»•æ§˜ã¸ã®æº–æ‹ ã«å•é¡ŒãŒã‚ã‚Šã¾ã™"
        
        return {
            "total_score": round(total_score, 2),
            "compliance_level": compliance_level,
            "status_message": status_message,
            "score_breakdown": {
                "protocol_version": round(scores[0], 2),
                "capabilities": round(scores[1], 2),
                "initialize": round(scores[2], 2),
                "tool_list": round(scores[3], 2),
                "tool_call": round(scores[4], 2),
                "error_handling": round(scores[5], 2),
                "message_format": round(scores[6], 2)
            }
        }


# Pytestãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹
class TestMCPProtocolCompliance:
    """MCP ãƒ—ãƒ­ãƒˆã‚³ãƒ«æº–æ‹ æ€§ã®ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    @pytest.fixture
    def tester(self):
        """ãƒ†ã‚¹ã‚¿ãƒ¼ã®ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
        return MCPProtocolCompatibilityTester()
    
    def test_protocol_version_is_supported(self, tester):
        """ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ãƒ†ã‚¹ãƒˆ"""
        result = tester.test_protocol_version_compliance()
        
        assert result["version_format_valid"], "ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å½¢å¼ãŒç„¡åŠ¹ã§ã™"
        assert result["is_supported_version"], "ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™"
        assert result["compliance_level"] in ["FULLY_COMPLIANT", "COMPLIANT", "LEGACY_COMPLIANT"]
    
    def test_server_capabilities_are_compliant(self, tester):
        """ã‚µãƒ¼ãƒãƒ¼æ©Ÿèƒ½ãŒæº–æ‹ ã—ã¦ã„ã‚‹ã“ã¨ã‚’ãƒ†ã‚¹ãƒˆ"""
        result = tester.test_server_capabilities_compliance()
        
        assert result["all_capabilities_compliant"], "ã‚µãƒ¼ãƒãƒ¼æ©Ÿèƒ½ã®æº–æ‹ æ€§ã«å•é¡ŒãŒã‚ã‚Šã¾ã™"
        assert result["compliance_score"] >= 80, f"æº–æ‹ ã‚¹ã‚³ã‚¢ãŒä½ã™ãã¾ã™: {result['compliance_score']}"
    
    @pytest.mark.asyncio
    async def test_initialize_request_handling(self, tester):
        """åˆæœŸåŒ–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆ"""
        result = await tester.test_initialize_request_handling()
        
        assert result["all_versions_supported"], "ä¸€éƒ¨ã®ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“"
        assert len(result["supported_versions"]) >= 2, "ååˆ†ãªãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“"
    
    @pytest.mark.asyncio
    async def test_tool_list_compliance(self, tester):
        """ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã®æº–æ‹ æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        result = await tester.test_tool_list_compliance()
        
        assert result["tools_list_valid"], "ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆãŒç„¡åŠ¹ã§ã™"
        assert result["tools_returned"] >= 3, "æœŸå¾…ã•ã‚Œã‚‹ãƒ„ãƒ¼ãƒ«æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
        assert result["all_tools_valid"], "ä¸€éƒ¨ã®ãƒ„ãƒ¼ãƒ«ãŒç„¡åŠ¹ã§ã™"
    
    @pytest.mark.asyncio
    async def test_tool_call_compliance(self, tester):
        """ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®æº–æ‹ æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        result = await tester.test_tool_call_compliance()
        
        assert result["all_validations_passed"], "ãƒ„ãƒ¼ãƒ«å¼•æ•°ã®æ¤œè¨¼ã«å•é¡ŒãŒã‚ã‚Šã¾ã™"
        assert len(result["validation_errors"]) == 0, f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {result['validation_errors']}"
    
    def test_error_handling_compliance(self, tester):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æº–æ‹ æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        result = tester.test_error_handling_compliance()
        
        assert result["all_errors_handled"], "ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã«å•é¡ŒãŒã‚ã‚Šã¾ã™"
        assert result["correct_error_messages"], "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"
    
    def test_message_format_compliance(self, tester):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã®æº–æ‹ æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        result = tester.test_message_format_compliance()
        
        assert result["all_formats_compliant"], "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã®æº–æ‹ æ€§ã«å•é¡ŒãŒã‚ã‚Šã¾ã™"
        assert result["compliance_percentage"] == 100, "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã®æº–æ‹ ç‡ãŒ100%ã§ã¯ã‚ã‚Šã¾ã›ã‚“"
    
    @pytest.mark.asyncio
    async def test_comprehensive_compliance(self, tester):
        """åŒ…æ‹¬çš„ãªæº–æ‹ æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
        result = await tester.run_comprehensive_compliance_test()
        
        overall = result["overall_assessment"]
        assert overall["total_score"] >= 85, f"å…¨ä½“çš„ãªæº–æ‹ ã‚¹ã‚³ã‚¢ãŒä½ã™ãã¾ã™: {overall['total_score']}"
        assert overall["compliance_level"] in ["EXCELLENT", "GOOD"], f"æº–æ‹ ãƒ¬ãƒ™ãƒ«ãŒä¸ååˆ†ã§ã™: {overall['compliance_level']}"


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ” MCP ãƒ—ãƒ­ãƒˆã‚³ãƒ«æº–æ‹ æ€§ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹...")
    print("=" * 60)
    
    tester = MCPProtocolCompatibilityTester()
    
    try:
        # åŒ…æ‹¬çš„ãªæº–æ‹ æ€§ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
        result = await tester.run_comprehensive_compliance_test()
        
        # çµæœã®è¡¨ç¤º
        print("ğŸ“Š MCP ãƒ—ãƒ­ãƒˆã‚³ãƒ«æº–æ‹ æ€§ãƒ†ã‚¹ãƒˆçµæœ")
        print("=" * 60)
        
        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        pv_result = result["protocol_version_compliance"]
        print(f"ğŸ“‹ ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³æº–æ‹ æ€§:")
        print(f"   ã‚µãƒ¼ãƒãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {pv_result['server_protocol_version']}")
        print(f"   å…¬å¼æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {pv_result['official_latest_version']}")
        print(f"   æº–æ‹ ãƒ¬ãƒ™ãƒ«: {pv_result['compliance_level']}")
        print(f"   æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {'âœ…' if pv_result['is_latest_version'] else 'âŒ'}")
        print()
        
        # ã‚µãƒ¼ãƒãƒ¼æ©Ÿèƒ½
        cap_result = result["server_capabilities_compliance"]
        print(f"âš™ï¸ ã‚µãƒ¼ãƒãƒ¼æ©Ÿèƒ½æº–æ‹ æ€§:")
        print(f"   æº–æ‹ ã‚¹ã‚³ã‚¢: {cap_result['compliance_score']:.1f}%")
        print(f"   å…¨æ©Ÿèƒ½æº–æ‹ : {'âœ…' if cap_result['all_capabilities_compliant'] else 'âŒ'}")
        print()
        
        # ãƒ„ãƒ¼ãƒ«é–¢é€£
        tool_list_result = result["tool_list_compliance"]
        tool_call_result = result["tool_call_compliance"]
        print(f"ğŸ”§ ãƒ„ãƒ¼ãƒ«æº–æ‹ æ€§:")
        print(f"   ãƒ„ãƒ¼ãƒ«æ•°: {tool_list_result['tools_returned']}")
        print(f"   ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆæœ‰åŠ¹: {'âœ…' if tool_list_result['all_tools_valid'] else 'âŒ'}")
        print(f"   ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æœ‰åŠ¹: {'âœ…' if tool_call_result['all_validations_passed'] else 'âŒ'}")
        print()
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        error_result = result["error_handling_compliance"]
        print(f"ğŸš¨ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æº–æ‹ æ€§:")
        print(f"   ã‚¨ãƒ©ãƒ¼å‡¦ç†: {'âœ…' if error_result['all_errors_handled'] else 'âŒ'}")
        print(f"   ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {'âœ…' if error_result['correct_error_messages'] else 'âŒ'}")
        print()
        
        # å…¨ä½“è©•ä¾¡
        overall = result["overall_assessment"]
        print(f"ğŸ“ˆ å…¨ä½“è©•ä¾¡:")
        print(f"   ç·åˆã‚¹ã‚³ã‚¢: {overall['total_score']:.2f}/100")
        print(f"   æº–æ‹ ãƒ¬ãƒ™ãƒ«: {overall['compliance_level']}")
        print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {overall['status_message']}")
        print()
        
        # ã‚¹ã‚³ã‚¢å†…è¨³
        print(f"ğŸ“Š ã‚¹ã‚³ã‚¢å†…è¨³:")
        breakdown = overall["score_breakdown"]
        for category, score in breakdown.items():
            print(f"   {category}: {score:.2f}")
        print()
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        import datetime
        report_filename = f"mcp_protocol_compliance_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_filename}")
        
        # çµ‚äº†ã‚³ãƒ¼ãƒ‰ã®æ±ºå®š
        if overall["compliance_level"] in ["EXCELLENT", "GOOD"]:
            print("âœ… MCP ãƒ—ãƒ­ãƒˆã‚³ãƒ«æº–æ‹ æ€§ãƒ†ã‚¹ãƒˆå®Œäº† - åˆæ ¼")
            return 0
        else:
            print("âŒ MCP ãƒ—ãƒ­ãƒˆã‚³ãƒ«æº–æ‹ æ€§ãƒ†ã‚¹ãƒˆå®Œäº† - æ”¹å–„ãŒå¿…è¦")
            return 1
            
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        import traceback
        traceback.print_exc()
        return 2


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # éåŒæœŸå®Ÿè¡Œ
    import sys
    exit_code = asyncio.run(main())
    sys.exit(exit_code)